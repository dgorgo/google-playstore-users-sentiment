{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3fd630d",
   "metadata": {},
   "source": [
    "### IMPORT DEPENDENCIES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adea12f",
   "metadata": {},
   "source": [
    "Importing all necessary modules needed for this project:\n",
    "- numpy for linear algebra and numerical transformations.\n",
    "- pandas for data processing and I/O.\n",
    "- matplotlib for data visualization.\n",
    "- seaborn for more advanced data visualizations.\n",
    "- scikitlearn for modeling and transformations.\n",
    "- beautiful soup for text parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17e1e245",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'warnings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-46ccf09ea771>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'warnings' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5724b0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('googleplaystore_user_reviews.csv')\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c0a2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b2c96b",
   "metadata": {},
   "source": [
    "### DATA PROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c19127",
   "metadata": {},
   "source": [
    "OBJECTIVES:\n",
    "- find missing values.\n",
    "- drop missing values.\n",
    "- encode labels manually and add to new column \"Target\".\n",
    "- keep only the necessary columns ie: \"Translated_Review\" and \"Target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1233c28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbcca20",
   "metadata": {},
   "source": [
    "The missing values are a lot but since we have a total entry of 64k+, we can still drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbd4777",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd7fcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60928ec",
   "metadata": {},
   "source": [
    "We still have 37k+ entries to work with which is good too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fdfc29",
   "metadata": {},
   "source": [
    "LABEL ENCODING:\n",
    "To preprocess we use the ‘sentiment’ column in the data frame to have scores ranging from ‘0 ‘to ‘2 ‘where ‘0’ means a negative review, ‘1’ means a neutral review and ‘2’ means a positive review. It is similar to encoding in python but here we don’t use any in-built function but we explicitly run a for loop where and create a new list and append the values to the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c150d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode labels manually\n",
    "\n",
    "def to_target(Sentiment):\n",
    "  Sentiment = str(Sentiment)\n",
    "\n",
    "  if Sentiment == \"Positive\":\n",
    "    return 2\n",
    "  elif Sentiment == \"Neutral\":\n",
    "    return 1\n",
    "  else:\n",
    "    return 0\n",
    "    \n",
    "data['Target'] = data.Sentiment.apply(to_target)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3416290f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize label\n",
    "\n",
    "sns.countplot(data[\"Target\"])\n",
    "plt.xlabel('Reviews', color = 'red')\n",
    "plt.ylabel('Count', color = 'red')\n",
    "plt.xticks([0,1,2],['Negative','Neutral','Positive'])\n",
    "plt.title('COUNT PLOT', color = 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c0ed36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create final dataset\n",
    "\n",
    "final_dataset = data[['Translated_Review','Target']]\n",
    "final_dataset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a089ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset[\"Target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a8efb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9afb7d1",
   "metadata": {},
   "source": [
    "OBSERVATION: Now if we print the ‘final_dataset’ and find the shape we come to know that there are 37,427 rows and only 2 columns. From the final_dataset, we find out the number of positive reviews is 23998 entries and the number of negative reviews is found to be 5158. There is a very large difference between the positive and negative reviews. Hence, there are more chances for the data to overfit if we directly try to build the model.\n",
    "\n",
    "Therefore, we have to choose only a few entries from the final_datset to avoid overfitting. So from various trials, I have found that the optimal value for the number of reviews to be considered is 5000. Hence I create two new variables ‘data_p’ and ‘data_n’ and store randomly any 5000 positive and negative reviews in the variables respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae3a96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datap = []\n",
    "# datan = []\n",
    "# dataneu = []\n",
    "\n",
    "# for i in final_dataset['Target']:\n",
    "#     if i == 2:                              \n",
    "#         datap.append(i)\n",
    "#     if i == 1:\n",
    "#         dataneu.append(i)\n",
    "#     if i == 0:\n",
    "#         datan.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ee55b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datap = pd.DataFrame(datap)\n",
    "# datan = pd.DataFrame(datan)\n",
    "# dataneu = pd.DataFrame(dataneu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b244bdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_p = datap.iloc[np.random.randint(1,23998,5000), :]\n",
    "# data_n = datan.iloc[np.random.randint(1, 8271,5000), :]\n",
    "# data_neu = dataneu.iloc[np.random.randint(1, 5158,5000), :]\n",
    "\n",
    "# len(data_n), len(data_p), len(data_neu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04e2a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.concat([data_p, data_n, data_neu])\n",
    "# len(data)\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db580716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_dataset['Target'].append(data)\n",
    "\n",
    "# sns.countplot(final_dataset['Target'])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e38a20",
   "metadata": {},
   "source": [
    "### TOKENIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d83eb64",
   "metadata": {},
   "source": [
    "If we see the data then we can find that there are a few HTML tags since the data was originally fetched from real e-commerce sites. Hence we can find that there are tags present which is to be removed as they are not necessary for the sentiment analysis. we use the BeautifulSoup function which uses the ‘html.parser’ and we can easily remove the unwanted tags from the reviews. To perform the task I create a new column named ‘review’ which stores the parsed text and I drop the column named ‘translated_review’ to avoid redundancy. I have performed the above task using a function named ‘strip_html’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4863b91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    \n",
    "    return soup.get_text()\n",
    "\n",
    "final_dataset['Review'] = final_dataset['Translated_Review'].apply(strip_html)\n",
    "final_dataset = final_dataset.drop('Translated_Review',axis=1)\n",
    "final_dataset.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5515a482",
   "metadata": {},
   "source": [
    "Before directly jumping to building the model we need, to do a small task. We know that for humans to classify the sentiment we need articles, determinants, conjunctions, punctuation marks, etc, as we can clearly understand and then classify the review.\n",
    "\n",
    "But this is not the case with machines. So they don’t actually need these to classify the sentiment rather they just get confused literally if they are present. So to perform this task like any other sentiment analysis we need to use the ‘nltk’ library.\n",
    "\n",
    "NLTK stands for ‘Natural Language Processing Toolkit’. This is one of the core libraries to perform Sentiment Analysis or any text-based ML Projects. So with the help of this library, I am going first remove the punctuation marks and then remove the words which do not add a sentiment to the text. First I use a function named ‘punc_clean’ which removes the punctuation marks from every review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436091a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def punc_clean(text):\n",
    "    \n",
    "    import string as st\n",
    "    a = [w for w in text if w not in st.punctuation]\n",
    "    \n",
    "    return ''.join(a)\n",
    "\n",
    "final_dataset['Review'] = final_dataset['Review'].apply(punc_clean)\n",
    "final_dataset.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b942a40",
   "metadata": {},
   "source": [
    "Now, next we have to remove the words which don’t add a sentiment to the sentence. Such words are called the ‘stopwords’. If we go through the list of the stopwords we can find that it contains the word ‘not’ as well. So it is necessary that we don’t remove the ‘not’ from the ‘review’ as it adds some value to the sentiment because it contributes to the negative sentiment. \n",
    "\n",
    "Hence we have to write the code in such a way that we remove other words except the ‘not’. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea2a101",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77848a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f6f302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopword(text):\n",
    "    stopword = nltk.corpus.stopwords.words('english')\n",
    "    stopword.remove('not')\n",
    "    \n",
    "    a = [w for w in nltk.word_tokenize(text) if w not in stopword]\n",
    "    \n",
    "    return ' '.join(a)\n",
    "\n",
    "final_dataset['Review'] = final_dataset['Review'].apply(remove_stopword)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b157be",
   "metadata": {},
   "source": [
    "### VECTORIZATION "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33a0661",
   "metadata": {},
   "source": [
    "The next motive is to assign each word in every review with a sentiment score. So to implement it we need to use another library from the ‘sklearn’ module which is the ‘TfidVectorizer’ which is present inside the ‘feature_extraction.text’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6947ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range = (1, 2), min_df = 1)\n",
    "vectorizer.fit(final_dataset['Review'])\n",
    "vectorizer_X = vectorizer.transform(final_dataset['Review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb2bc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "\n",
    "clf = model.fit(vectorizer_X, final_dataset['Target'])\n",
    "clf.score(vectorizer_X, final_dataset['Target']) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f71fd1",
   "metadata": {},
   "source": [
    "The score of the model we get around 96 – 97% as the dataset changes every time we run the code as we consider the data randomly. Hence we have successfully built our model that too with a good score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43646d70",
   "metadata": {},
   "source": [
    "### PREDICTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b49f8a9",
   "metadata": {},
   "source": [
    "So to clarify the performance of the model I have used two simple sentences “I love machine learning down to its complexities” and “I so hate data analysis” which clearly refer to positive and negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd206d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(vectorizer.transform([\"I love machine learning down to its complexities\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be10f9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(vectorizer.transform([\"I so hate data analysis\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019918e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
